# Airflow-Driven ETL Pipeline (Dockerized)

A production-style, Dockerized **Apache Airflow ETL pipeline** that orchestrates scheduled data ingestion from external APIs into **PostgreSQL**, demonstrating clean DAG design, containerized infrastructure, and scalable data engineering best practices.

---

## ğŸš€ Overview

This project showcases how to build and run an **end-to-end ETL pipeline** using:

* **Apache Airflow** for orchestration and scheduling
* **PostgreSQL** as the analytical datastore
* **Docker & Docker Compose** for local, reproducible environments
* **Modular Python code** for maintainability and extensibility

The pipeline is designed to run on a **fixed schedule**, fetch data from an external API, and persist it into a relational database.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ External API â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Airflow DAG        â”‚
â”‚ (PythonOperator)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL         â”‚
â”‚ (Docker Container) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* Airflow runs inside a Docker container
* PostgreSQL runs in a separate container
* Containers communicate via a Docker bridge network
* All orchestration logic lives inside Airflow DAGs

---

## ğŸ“ Project Structure

```
Airflow-Driven-ETL-Pipeline-Dockerized/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ orchestrator.py
â”‚
â”œâ”€â”€ api-request/
â”‚   â””â”€â”€ insert_records.py
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ airflow_init.sql
â”‚
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### Key Components

* **`orchestrator.py`** â€“ Airflow DAG definition
* **`insert_records.py`** â€“ API ingestion & database logic
* **`airflow_init.sql`** â€“ Database bootstrap script
* **`docker-compose.yaml`** â€“ Multi-container orchestration

---

## â±ï¸ Scheduling

* The DAG is configured to run **every minute**
* Uses Airflowâ€™s modern `schedule` parameter (Airflow 3+ compatible)
* Catchup is disabled to avoid historical backfills

---

## âš™ï¸ Tech Stack

| Component     | Technology             |
| ------------- | ---------------------- |
| Orchestration | Apache Airflow 3.x     |
| Database      | PostgreSQL 14          |
| Language      | Python 3.12            |
| Containers    | Docker, Docker Compose |
| Networking    | Docker Bridge Network  |

---

## ğŸ³ Running the Project Locally

### Prerequisites

* Docker
* Docker Compose
* Git

---

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/bhuwanesh340/Airflow-Driven-ETL-Pipeline-Dockerized.git
cd Airflow-Driven-ETL-Pipeline-Dockerized
```

---

### 2ï¸âƒ£ Start the Services

```bash
docker compose up -d
```

This will:

* Start PostgreSQL
* Initialize the database
* Start Airflow (Webserver + Scheduler)
* Auto-migrate Airflow metadata DB

---

### 3ï¸âƒ£ Access Airflow UI

```
http://localhost:8000
```

**Default credentials:**

```
Username: airflow
Password: airflow
```
<img width="1505" height="794" alt="image" src="https://github.com/user-attachments/assets/198e5a07-2af2-472a-bfb2-3b7ea43fe592" />

---

## ğŸ§ª Verifying the Pipeline

1. Open Airflow UI
2. Enable the **`orchestrator`** DAG
3. Trigger manually or wait for the scheduler
4. View logs to confirm:

   * API request success
   * Database connection
   * Data insertion

---

## ğŸ›‘ Stopping the Pipeline

To pause execution:

* Disable the DAG in the Airflow UI

To stop containers:

```bash
docker compose down
```

---

## ğŸ”’ Git & Data Safety

* **PostgreSQL runtime data is excluded from Git**
* Virtual environments are ignored
* Only source code and configuration are tracked

This keeps the repository:

* Lightweight
* Secure
* Production-friendly

---

## ğŸ§  Key Learnings Demonstrated

* Airflow DAG lifecycle & scheduling
* Docker networking between services
* Containerized Postgres connectivity
* Clean separation of orchestration and business logic
* Production-grade Git hygiene

---

## ğŸ“Œ Future Enhancements

* Add Airflow Connections & Variables
* Replace PythonOperator with TaskFlow API
* Add retries, SLAs, and alerting
* Introduce data validation & schema checks
* Add CI/CD pipeline
* Migrate to Kubernetes / Helm

---

## ğŸ‘¤ Author

**Bhuwanesh Tripathi**
Data Engineer | Data Scientist | Cloud & MLOps
ğŸ“Œ GitHub: [https://github.com/bhuwanesh340](https://github.com/bhuwanesh340)

---

## â­ If you find this useful

Give the repo a â­ â€” it helps and is appreciated!
